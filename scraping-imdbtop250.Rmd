---
title: "Web Scraping Example - IMDB Top 250 Movies"
author: Adrianne Bradford
date: February 25, 2019
output:
  html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE)
```

```{r cache=FALSE, echo=FALSE}
library("kableExtra")
```

##Installing XML and rvest
First, we need to load the package used in R for web scraping, rvest, and a package for parsing XML, called XML.
```{r cache=FALSE, warning=FALSE, message=FALSE}
if (!requireNamespace("XML", quietly = TRUE)) {
  install.packages("XML")
}
if (!requireNamespace("rvest", quietly = TRUE)) {
  install.packages("rvest")
}

library('XML')
library('rvest')
```

##The Website
We are going to scrape the page which is a listing of the top 250 movies on IMDB.  We will obtain the names of the movies, the URL for more information about the movie, the year of the movie, the rating, runtime, genre, and the star rating.
First, we set up a string variable, called "site" and save the URL of the website we will scrape to it.  This will become an argument we pass to an R function to return the HTML results we will parse for the data we seek.  In the next line we use the `read_html` function to perform this action.  
```{r cache=FALSE}
site <- "https://www.imdb.com/search/title?groups=top_250&sort=user_rating"
sitehtml <- read_html(site)
sitehtml
```
This doesn't look like a lot of output, but all of the code for the site is contained in these two objects - one for the head part of the page, the other for the body.  

Looking just at the body, we see more of the code, but again large sections of the page are contained in each of these items.
```{r}
html_node(sitehtml, 'body')
```

In order to find the pieces of the page we want, we need to dig into the source code and locate exactly where the pieces we need are and pick those out.

##Planning the Scrape
Now we review our website in a web browser to determine where in the HTML code the pieces we want to scrape are located.  This can be done by using the "Developer Tools" built into Google Chrome (similar tools available in Firefox as well) by right clicking on an item you want to scrape and clicking "Inspect."  Another option is to install the SelectorGadget extension for Google Chrome (https://selectorgadget.com/).  When that extension is enabled you can click and highlight items of interest on the webpage and see the html nodes they are located in.  For this first one we'll use the selectogGadget, as pictured below.

![](selecth3a.png)

With SelectorGadget enabled, we mouse over the name of the first movie, and the orange square that shows up tells us that the name of the movie is in the `h3 a` tag.  [h3 is header level 3, a is an anchor tag that usually precedes a link (href)]

## First Row of the Table
So first, let's pull back the first row of content using the SelectorGadget to identify the tag for the movie names to see what we get.  We'll use `html_node` to just get the first instance of the item on the page.
```{r}
firstrow <- html_node(sitehtml,'h3 a')
firstrow
html_text(firstrow)
```
This looks like the information we want, the name of the first movie on the list and the link to that movie's detailed page on IMDB.  Note that the link doesn't include the full link of the movie's page, starting with http://www.imdb.com, this is because it is a relative link, relative to your current position in the website.  If you were to link to that page from your own site, you would need to use an absolute link, with the full http://www.imdb.com included.

## Get all Rows
The function `html_node` pulled down one item, the first instance of what we were looking for, and it was the correct item.  If we want to pull down all of the items, we use the function `html_nodes` (note the plural) to get all the `<h3 a>` objects from the website.
```{r}
body_rows <- html_nodes(sitehtml, "h3 a")
## PRINT FIRST 4 ITEMS
body_rows[1:4]
```

We know there were only 50 movies, but there are 55 items in our list of body rows.  Looking at the tail of the list (the last 6 items) we confirm the last 5 are likely special links for ads/etc. at the bottom of the page.
```{r}
length(body_rows)
tail(body_rows)
```

## Getting just the bits we want
We have our listing of the first 50 movie titles and links, but they're stuck together in a cell with a bunch of html "stuff" around them too.  

We want to extract from our `body_rows` two lists with just the pieces we want to keep, one with the titles and one with the URLs.

We obtain the titles by using the `html_text` function on just our `body_rows` object (not the full webpage).  This function pulls out all the text that is not part of the HTML tags - it pulls everything between the open tag `<a>` and the close tag `</a>`  It pulls back 55 items, 5 of which are our blank items that are at the end of the page and we determined are not movies, so we trim those off the end of our list of titles.

```{r}
titles <- html_text(body_rows)
length(titles)
titles <- titles[1:50]
titles
```

We also need to capture the URLs separately. To do this we need to use the `html_attr` function to obtain the subset (attribute) of the `<a>` (anchor) object that contains the link `href=` and again only obtain the first 50 items.
```{r}
urls <- (html_attr(body_rows, "href"))[1:50]
urls
```

Now we have our first two vectors one that is the 50 titles, and one that is the 50 corresponding URLs.  Seems like a good time to create a dataframe to house these items.  As we collect our other pieces of data they can be added to the frame.  Adding new columns to a dataframe is easy, you put the name of the dataframe followed by the name you want for the column in brackets and quotes.

```{r}
imdb_top_250 = as.data.frame(titles, stringsAsFactors = FALSE)
imdb_top_250["URLs"] <- urls
```


__Here is how the first few items of our data.frame look at this point:__

`head(imdb_top_250)`


`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `


## MORE DATA!
The other items we want to obtain are:
the year of the movie, 
the rating, 
runtime, 
genre, 
and IMDB star rating.


For this we need to go back to the website and look at the HTML again.  This time we'll use the css selector for this item, obtained from the "inspect" option in Google Chrome.  We'll start with year.  I review the selector path when I inspect the year and determine that the year is within the span called "lister-item-year.text-muted.unbold."

![](yearimg.png)

Now we'll use the `html_node` function to pull back the first item to make sure we're pulling the right information.  Note that I'm calling my function again on the entire `sitehtml`, not `body_rows`.  `body_rows` was a specific subset of the full `sitehtml` that only contained the title information.

Here I am using the `%>%` operator to string together multiple functions.  This is called a pipe operator from the package `magrittr` which is built into rvest as both are created by the same developer.  This operator allows us to perform sequential actions without nesting functions inside each other, which makes our code more readable. 

Without the pipe operator, our code to perform the two actions would look like this with the function calls "nested" inside of each other:

`html_text(html_node(sitehtml, 'span.lister-item-year.text-muted.unbold'))`

This is less readable because the LAST function we're using is appearing FIRST in the code.

I first use `html_node` to obtain the node for this span item, then `html_text` to extract the text from that node.

```{r}
html_node(sitehtml, 'span.lister-item-year.text-muted.unbold') %>% html_text()
```

I see that I'm correctly getting the year of the first movie, so I update my call to obtain all of the nodes.

```{r}
html_nodes(sitehtml, 'span.lister-item-year.text-muted.unbold') %>% html_text()
```

Now we've figure out where our years are, we need to build up a call that will pull them, remove the parens, and save them to our dataframe, utilizing the `magrittr` pipe operator to add each piece sequentially.

To remove the parens we'll need to use string functions. So we'll load the `stringr` package.

```{r warning=FALSE, message=FALSE, cache=FALSE}
if (!requireNamespace("stringr", quietly = TRUE)) {
  install.packages("stringr")
}
library("stringr")
```

Here we add the function `str_extract` to the end of our set of actions.  `str_extract` takes an argument that is called a Regular Expression (regex) which is like a pattern matcher that tells the computer what piece of a string of text we want to extract (keep).  Here we want the four numbers without the parentheses.  `\\d` tells the computer we want a number and adding `{4}` says we want 4 numbers in a row.
```{r}
html_nodes(sitehtml, 'span.lister-item-year.text-muted.unbold') %>% html_text() %>% str_extract("\\d{4}")
```

Now that we have the years formatted in the way we want them, we simply save it to our dataframe in a column called "year."

```{r}
imdb_top_250["year"] <- html_nodes(sitehtml, 'span.lister-item-year.text-muted.unbold') %>% html_text() %>% str_extract("\\d{4}")
```

__Here is how the first 6 rows look now with the years added:__

`head(imdb_top_250)`

`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `



We'll quickly build the rest of our columns...

__It's your turn to try.__  Inspect the website and figure out the selector path for the MPAA rating of the movie (PG, PG-13, R, etc.).  Resist the urge to peek below!!!

## FIND THE RATING USING INSPECT IN CHROME
![](https://media.giphy.com/media/g4jDE1JnpUNaw/giphy.gif)

## AND THE ANSWER IS....

![](rating.png)

## The Rating
We've found that the MPAA rating is contained in a span called certificate.  I quickly built the string of `html_nodes` followed by `html_text` to pull out these ratings and add them to our dataframe.
```{r}
imdb_top_250["rating"] <- html_nodes(sitehtml, 'span.certificate') %>% html_text()
```

__Revisiting the top 6 rows in the dataframe:__

`head(imdb_top_250)`

`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `

Now I'll quickly pull 3 more items into our dataframe:

## The runtime
Here for the regex we want just the number`\\d`, of any length `*`, and not the text that says "min."  I'll also convert the number as text into a number in case we want to do math with these numbers at some point.
```{r}
imdb_top_250["runtime"] <- html_nodes(sitehtml, 'span.runtime') %>% html_text() %>% str_extract("\\d*") %>% as.numeric()
```

## Genre
Some movies have more than one genre, but right now we'll just keep them as one text string. We do have to trim the leading whitespace using `str_trim` also from the `stringr` package to remove any extra whitespace.
```{r}
imdb_top_250["genres"] <- html_nodes(sitehtml, 'span.genre') %>% html_text() %>% str_trim()
```

## IMDB star rating
We'll pull out the div with the rating, then the rating itself, which is the text inside the "strong" tag within that div.  When we scrape the site, these numbers are stored as text, so we'll use "as.numeric" to covert it to a number.
```{r}
imdb_top_250["stars"] <- html_nodes(sitehtml, 'div.inline-block.ratings-imdb-rating') %>% html_node ("strong") %>% html_text() %>% as.numeric()
```

At this point we have the 7 pieces of information (columns) collected for the 50 movies on the first page.

## ALL THE COLUMNS

__Let's take a peek at the first few items in our dataframe now:__

`head(imdb_top_250)`

`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `


To collect the information for the other 200 movies, we will have to navigate to the next page and repeat the process.  However, this time we already know where our information is, and we already have our dataframe set up, so we'll simply append these results to the bottom.

## Navigate to the next page
In order to navigate within the site, we need to first set up a session.  Remember we saved the URL of the page of IMDB's top 250 in a variable called "site."
```{r cache=FALSE}
sess <- html_session(site)
is.session(sess)
```

```{r cache=FALSE, message=TRUE}
sess <- sess %>% follow_link("Next")
```

With the session activated, we don't need to use the `read_html` step.  Let's read the list number of the first item on the page to ensure we're on items 51 through 100.
```{r}
html_node(sess, "span.lister-item-index.unbold.text-primary") %>% html_text()
```

Now we're quickly going to scrape the 7 columns of data as we did above, saving each as a vector.  These are the code pieces we built above.  I've added "lister-item-header" as a qualifier to `<h3 a>` in order to pull only the movie titles and not the additional `<h3 a>` items from the bottom of the page (the extra 5 items we had removed above).
```{r}
titles <- html_nodes(sess, "h3.lister-item-header a") %>% html_text()
urls <- html_nodes(sess, "h3.lister-item-header a") %>% html_attr("href")
years <- html_nodes(sess, 'span.lister-item-year.text-muted.unbold') %>% html_text() %>% str_extract("\\d{4}")
ratings <- html_nodes(sess, 'span.certificate') %>% html_text()
runtimes <- html_nodes(sess, 'span.runtime') %>% html_text() %>% str_extract("\\d*") %>% as.integer()
genres <- html_nodes(sess, 'span.genre') %>% html_text() %>% str_trim()
stars <- html_nodes(sess, 'div.inline-block.ratings-imdb-rating') %>% html_node ("strong") %>% html_text() %>% as.numeric()
```

Now we need to bind the columns together creating a data.frame called page with just items 51-100.
```{r error=TRUE}
page <- data.frame(titles, urls, years, ratings, runtimes, genres, stars)
```

## OOPS - we got an error.

```{r }
length(titles) 
length(urls)
length(years)
length(ratings) 
length(runtimes) 
length(genres)
length(stars)
```

It looks like two of the movies on this page have no rating listed, and the object the rating is supposed to be in is not even present. Because the object isn't present, we don't even get a blank, therefore having only 48 ratings with 50 movies.  Our method of scraping was naive to this issue.  We're going to slightly reformat the scraper to read each movie one by one and pick out the pieces we need.

We will select the larger node that contains the information about each movie, then pick out the attributes that correspond to each of our 7 items from that larger node.  When those nodes are not present it will automatically include an NA value in that row.

First, create a completely empty dataframe... 
```{r}
page <- data.frame()
```

...and read each of the movies off of the page into a vector where each element is the HTML for the entire movie "block."

![](entiremovie.png)



```{r}
movies_html <- html_nodes(sess, '.mode-advanced')
```


Check that reading within the first element of the movie vector we can read the node we need using the same node descriptors from above.
```{r}
movies_html[1] %>% html_node("h3.lister-item-header a")
```

So instead of scraping 50 titles, then 50 URLs, etc. from the website, we'll scrape the block containing each movie, then 1 by 1 we'll scrape the title, URL, etc. from each movie block.  For this we'll loop through the 50 movie blocks on the page to read each of the 7 nodes and append it to our empty dataframe. This way when an item returns an NA (it's not found in that movie block), it is included in that row as an NA.
```{r}
len <- length(movies_html) # get the length of the movie blocks vector as a variable
for (i in 1:len) { # loop through each movie block one by one
  #set the 7 elements for this movie
  title <- movies_html[i] %>% html_node("h3.lister-item-header a") %>% html_text()
  url <- movies_html[i] %>% html_node("h3.lister-item-header a") %>% html_attr("href")
  year <- movies_html[i] %>% html_node('span.lister-item-year.text-muted.unbold') %>% html_text() %>% str_extract("\\d{4}")
  rating <- movies_html[i] %>% html_node('span.certificate') %>% html_text()
  runtime <- movies_html[i] %>% html_node('span.runtime') %>% html_text() %>% str_extract("\\d*") %>% as.integer()
  genre <- movies_html[i] %>% html_node('span.genre') %>% html_text() %>% str_trim()
  star_rating <- movies_html[i] %>% html_node('div.inline-block.ratings-imdb-rating') %>% html_node ("strong") %>% html_text() %>% as.numeric()
  
  #create the row for this movie by column binding (cbind) the 7 items
  movie_row <- cbind(title, url, year, rating, runtime, genre, star_rating)
  #append the new row to the page df
  page <- rbind (page, movie_row)
}
```


__Let's take a peek at the last few items in our dataframe from this page:__

`tail(page)`

`r tail(page) %>% kable(row.names=FALSE) %>% kable_styling("striped", full_width = F) `


Now that we have the items from the second page (movies 51-100), we'll append that to our first 50 items to create our dataframe of the first 100 items.

In order to do this, the column names in both dfs need to match. (I could have used the same column names, BTW, but I was being lazy while coding.) It's easy to fix using the `colnames` function.

```{r}
colnames(page) <- colnames(imdb_top_250)
```

```{r}
imdb_top_250 <- rbind(imdb_top_250, page)
dim(imdb_top_250)
```

__Now we can look at the first items and the last items of our imdb_top_250 dataframe to ensure that it starts with the first items from the first page and ends with the last items from the second page we just scraped:__

__HEAD:__
`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `

__TAIL:__
`r tail(imdb_top_250) %>% kable() %>% kable_styling("striped", full_width = F) `


Finally, we can wrap that "next page process" into one big loop to get the last 3 pages we need.
```{r cache=FALSE}
for(i in 1:3){
  sess <- sess %>% follow_link("Next")
  page <- data.frame()
  movies_html <- html_nodes(sess, '.mode-advanced')
  len <- length(movies_html) # get the length of the movie blocks vector as a variable
  for (i in 1:len) { # loop through each movie block one by one
    #set the 7 elements for this movie
    title <- movies_html[i] %>% html_node("h3.lister-item-header a") %>% html_text()
    url <- movies_html[i] %>% html_node("h3.lister-item-header a") %>% html_attr("href")
    year <- movies_html[i] %>% html_node('span.lister-item-year.text-muted.unbold') %>% html_text() %>% str_extract("\\d{4}")
    rating <- movies_html[i] %>% html_node('span.certificate') %>% html_text()
    runtime <- movies_html[i] %>% html_node('span.runtime') %>% html_text() %>% str_extract("\\d*") %>% as.integer()
    genre <- movies_html[i] %>% html_node('span.genre') %>% html_text() %>% str_trim()
    star_rating <- movies_html[i] %>% html_node('div.inline-block.ratings-imdb-rating') %>% html_node ("strong") %>% html_text() %>% as.numeric()
    
    #create the row for this movie by column binding (cbind) the 7 items
    movie_row <- cbind(title, url, year, rating, runtime, genre, star_rating)
    #append the new row to the page df
    page <- rbind (page, movie_row)
  }
  colnames(page) <- colnames(imdb_top_250)
  imdb_top_250 <- rbind(imdb_top_250, page)
}
dim(imdb_top_250) # print final dimensions
```

__HEAD:__
`r head(imdb_top_250) %>% kable(row.names=TRUE) %>% kable_styling("striped", full_width = F) `

__TAIL:__
`r tail(imdb_top_250) %>% kable() %>% kable_styling("striped", full_width = F) `



Now that we have our data we can do things like print just the rows that are classified as Comedies.
```{r}
comedies <- imdb_top_250[grep("Comedy", imdb_top_250$genre), ]
```

__`head(comedies)`__
`r head(comedies) %>% kable() %>% kable_styling("striped", full_width = F) `

Or we can find the distribution of runtimes:
```{r}
summary(as.numeric(imdb_top_250$runtime))
```

```{r}
hist(as.numeric(imdb_top_250$runtime), main="Runtimes of IMDB Top Rated Movies", xlab="Runtime (in minutes)", col="darkmagenta", breaks=40, xlim=c(40,330))
```